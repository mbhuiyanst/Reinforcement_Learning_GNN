{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a9acbe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function Viewer.__del__ at 0x000002C5432659E0>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\mrhbhuiyan\\AppData\\Local\\anaconda3\\Lib\\site-packages\\gym\\envs\\classic_control\\rendering.py\", line 152, in __del__\n",
      "    self.close()\n",
      "  File \"C:\\Users\\mrhbhuiyan\\AppData\\Local\\anaconda3\\Lib\\site-packages\\gym\\envs\\classic_control\\rendering.py\", line 71, in close\n",
      "    self.window.close()\n",
      "  File \"C:\\Users\\mrhbhuiyan\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pyglet\\window\\win32\\__init__.py\", line 299, in close\n",
      "    super(Win32Window, self).close()\n",
      "  File \"C:\\Users\\mrhbhuiyan\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pyglet\\window\\__init__.py\", line 823, in close\n",
      "    app.windows.remove(self)\n",
      "  File \"C:\\Users\\mrhbhuiyan\\AppData\\Local\\anaconda3\\Lib\\_weakrefset.py\", line 113, in remove\n",
      "    self.data.remove(ref(item))\n",
      "KeyError: <weakref at 0x000002C543207B50; to 'Win32Window' at 0x000002C541D69BD0>\n",
      "Exception ignored in: <function Viewer.__del__ at 0x000002C5432659E0>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\mrhbhuiyan\\AppData\\Local\\anaconda3\\Lib\\site-packages\\gym\\envs\\classic_control\\rendering.py\", line 152, in __del__\n",
      "    self.close()\n",
      "  File \"C:\\Users\\mrhbhuiyan\\AppData\\Local\\anaconda3\\Lib\\site-packages\\gym\\envs\\classic_control\\rendering.py\", line 71, in close\n",
      "    self.window.close()\n",
      "  File \"C:\\Users\\mrhbhuiyan\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pyglet\\window\\win32\\__init__.py\", line 299, in close\n",
      "    super(Win32Window, self).close()\n",
      "  File \"C:\\Users\\mrhbhuiyan\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pyglet\\window\\__init__.py\", line 823, in close\n",
      "    app.windows.remove(self)\n",
      "  File \"C:\\Users\\mrhbhuiyan\\AppData\\Local\\anaconda3\\Lib\\_weakrefset.py\", line 113, in remove\n",
      "    self.data.remove(ref(item))\n",
      "KeyError: <weakref at 0x000002C54A4130B0; to 'Win32Window' at 0x000002C540A246D0>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0, Reward: -4.490750048409568, Average Loss: 0.0000\n",
      "\n",
      "Q-Values:\n",
      "State 1: Q1 = -0.1499, Q2 = 0.1949\n",
      "State 2: Q1 = -0.1237, Q2 = 0.1883\n",
      "State 3: Q1 = -0.1543, Q2 = 0.1963\n",
      "State 4: Q1 = -0.0910, Q2 = 0.2086\n",
      "State 5: Q1 = -0.1081, Q2 = 0.1861\n",
      "State 6: Q1 = -0.1515, Q2 = 0.1953\n",
      "State 7: Q1 = -0.1413, Q2 = 0.1924\n",
      "State 8: Q1 = -0.0921, Q2 = 0.2107\n",
      "State 9: Q1 = -0.1390, Q2 = 0.1916\n",
      "State 10: Q1 = -0.0977, Q2 = 0.2156\n",
      "Episode: 1, Reward: -4.091652869767966, Average Loss: 0.0000\n",
      "Episode: 2, Reward: -2.923813660671167, Average Loss: 0.0047\n",
      "Episode: 3, Reward: -0.0593699215355812, Average Loss: 0.0000\n",
      "Episode: 4, Reward: -0.0105792447586813, Average Loss: 0.0000\n",
      "Episode: 5, Reward: -0.005457314034212334, Average Loss: 0.0000\n",
      "Episode: 6, Reward: -0.0065588134707909775, Average Loss: 0.0000\n",
      "Episode: 7, Reward: -0.006004398009048482, Average Loss: 0.0000\n",
      "Episode: 8, Reward: -0.0018671802238450215, Average Loss: 0.0000\n",
      "Episode: 9, Reward: -0.0015125772916744903, Average Loss: 0.0000\n",
      "Episode: 10, Reward: -0.0073156669658472635, Average Loss: 0.0000\n",
      "\n",
      "Q-Values:\n",
      "State 1: Q1 = -0.0850, Q2 = -0.0861\n",
      "State 2: Q1 = -0.0850, Q2 = -0.0861\n",
      "State 3: Q1 = -0.0851, Q2 = -0.0861\n",
      "State 4: Q1 = -0.0849, Q2 = -0.0862\n",
      "State 5: Q1 = -0.0849, Q2 = -0.0862\n",
      "State 6: Q1 = -0.0850, Q2 = -0.0861\n",
      "State 7: Q1 = -0.0851, Q2 = -0.0861\n",
      "State 8: Q1 = -0.0847, Q2 = -0.0862\n",
      "State 9: Q1 = -0.0849, Q2 = -0.0862\n",
      "State 10: Q1 = -0.0848, Q2 = -0.0862\n",
      "Episode: 11, Reward: -0.004677513262462512, Average Loss: 0.0000\n",
      "Episode: 12, Reward: -0.006900154527277765, Average Loss: 0.0000\n",
      "Episode: 13, Reward: -0.001845231804066284, Average Loss: 0.0000\n",
      "Episode: 14, Reward: -0.0016524167061897773, Average Loss: 0.0000\n",
      "Episode: 15, Reward: -0.0008319063859804905, Average Loss: 0.0000\n",
      "Episode: 16, Reward: -0.0015804780533248963, Average Loss: 0.0000\n",
      "Episode: 17, Reward: -0.0008090012669151441, Average Loss: 0.0000\n",
      "Episode: 18, Reward: -0.0038085307616571597, Average Loss: 0.0000\n",
      "Episode: 19, Reward: -0.0013820795634484765, Average Loss: 0.0000\n",
      "Episode: 20, Reward: -0.0010162777662604068, Average Loss: 0.0000\n",
      "\n",
      "Q-Values:\n",
      "State 1: Q1 = -0.0735, Q2 = -0.0745\n",
      "State 2: Q1 = -0.0735, Q2 = -0.0745\n",
      "State 3: Q1 = -0.0735, Q2 = -0.0745\n",
      "State 4: Q1 = -0.0735, Q2 = -0.0745\n",
      "State 5: Q1 = -0.0735, Q2 = -0.0745\n",
      "State 6: Q1 = -0.0735, Q2 = -0.0745\n",
      "State 7: Q1 = -0.0735, Q2 = -0.0745\n",
      "State 8: Q1 = -0.0735, Q2 = -0.0745\n",
      "State 9: Q1 = -0.0735, Q2 = -0.0745\n",
      "State 10: Q1 = -0.0735, Q2 = -0.0745\n",
      "Episode: 21, Reward: -0.0007635038566797569, Average Loss: 0.0000\n",
      "Episode: 22, Reward: -0.001240847037327139, Average Loss: 0.0000\n",
      "Episode: 23, Reward: -0.0010410163409752425, Average Loss: 0.0000\n",
      "Episode: 24, Reward: -0.002779239945343903, Average Loss: 0.0000\n",
      "Episode: 25, Reward: -0.001624180948241692, Average Loss: 0.0000\n",
      "Episode: 26, Reward: -0.0007208475236139402, Average Loss: 0.0000\n",
      "Episode: 27, Reward: -0.0008561938774527499, Average Loss: 0.0000\n",
      "Episode: 28, Reward: -0.00043867315219219786, Average Loss: 0.0000\n",
      "Episode: 29, Reward: -0.0013275181821183458, Average Loss: 0.0000\n",
      "Episode: 30, Reward: -0.002378753459410747, Average Loss: 0.0000\n",
      "\n",
      "Q-Values:\n",
      "State 1: Q1 = -0.0785, Q2 = -0.0781\n",
      "State 2: Q1 = -0.0785, Q2 = -0.0782\n",
      "State 3: Q1 = -0.0785, Q2 = -0.0782\n",
      "State 4: Q1 = -0.0785, Q2 = -0.0782\n",
      "State 5: Q1 = -0.0785, Q2 = -0.0782\n",
      "State 6: Q1 = -0.0785, Q2 = -0.0782\n",
      "State 7: Q1 = -0.0785, Q2 = -0.0782\n",
      "State 8: Q1 = -0.0785, Q2 = -0.0782\n",
      "State 9: Q1 = -0.0785, Q2 = -0.0782\n",
      "State 10: Q1 = -0.0785, Q2 = -0.0782\n",
      "Episode: 31, Reward: -0.0010970314055872923, Average Loss: 0.0000\n",
      "Episode: 32, Reward: -0.0036675813894097966, Average Loss: 0.0000\n",
      "Episode: 33, Reward: -0.0004127874344092777, Average Loss: 0.0000\n",
      "Episode: 34, Reward: -0.0036610590628350236, Average Loss: 0.0000\n",
      "Episode: 35, Reward: -0.000887504679510462, Average Loss: 0.0000\n",
      "Episode: 36, Reward: -0.0002447151440340685, Average Loss: 0.0000\n",
      "Episode: 37, Reward: -0.0004034018364779024, Average Loss: 0.0000\n",
      "Episode: 38, Reward: -0.0005918444799559618, Average Loss: 0.0000\n",
      "Episode: 39, Reward: -0.0006431419773472908, Average Loss: 0.0000\n",
      "Episode: 40, Reward: -0.001172359302998652, Average Loss: 0.0000\n",
      "\n",
      "Q-Values:\n",
      "State 1: Q1 = -0.0741, Q2 = -0.0738\n",
      "State 2: Q1 = -0.0741, Q2 = -0.0738\n",
      "State 3: Q1 = -0.0741, Q2 = -0.0738\n",
      "State 4: Q1 = -0.0741, Q2 = -0.0738\n",
      "State 5: Q1 = -0.0741, Q2 = -0.0738\n",
      "State 6: Q1 = -0.0741, Q2 = -0.0738\n",
      "State 7: Q1 = -0.0742, Q2 = -0.0738\n",
      "State 8: Q1 = -0.0742, Q2 = -0.0738\n",
      "State 9: Q1 = -0.0741, Q2 = -0.0738\n",
      "State 10: Q1 = -0.0741, Q2 = -0.0738\n",
      "Episode: 41, Reward: -0.0005567691086823487, Average Loss: 0.0000\n",
      "Episode: 42, Reward: -0.00045370858301442486, Average Loss: 0.0000\n",
      "Episode: 43, Reward: -0.0003592155553546523, Average Loss: 0.0000\n",
      "Episode: 44, Reward: -0.0011365869493564996, Average Loss: 0.0000\n",
      "Episode: 45, Reward: -0.00044125521220884716, Average Loss: 0.0000\n",
      "Episode: 46, Reward: -0.00035332160352799784, Average Loss: 0.0000\n",
      "Episode: 47, Reward: -0.00040814270386211834, Average Loss: 0.0000\n",
      "Episode: 48, Reward: -0.0010533352210298718, Average Loss: 0.0000\n",
      "Episode: 49, Reward: -0.0026234137262061545, Average Loss: 0.0000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import gym\n",
    "import random\n",
    "\n",
    "# Define the SAC agent\n",
    "class SACAgent:\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=(256, 256), alpha=0.2, lr=3e-4):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.alpha = alpha\n",
    "\n",
    "        # Define the Q-networks for both Q1 and Q2\n",
    "        self.q1_network = self.build_q_network(hidden_dim)\n",
    "        self.q2_network = self.build_q_network(hidden_dim)\n",
    "\n",
    "        # Target Q-networks for soft updates\n",
    "        self.target_q1_network = self.build_q_network(hidden_dim)\n",
    "        self.target_q2_network = self.build_q_network(hidden_dim)\n",
    "        self.update_targets(self.target_q1_network, self.q1_network, tau=1)\n",
    "        self.update_targets(self.target_q2_network, self.q2_network, tau=1)\n",
    "\n",
    "        # Policy network\n",
    "        self.policy_network = self.build_policy_network(hidden_dim)\n",
    "\n",
    "        # Define Optimizers for Q-networks and policy network\n",
    "        self.q1_optimizer = optim.Adam(self.q1_network.parameters(), lr=lr)\n",
    "        self.q2_optimizer = optim.Adam(self.q2_network.parameters(), lr=lr)\n",
    "        self.policy_optimizer = optim.Adam(self.policy_network.parameters(), lr=lr)\n",
    "\n",
    "    # Define or build  a Q-network architecture\n",
    "    def build_q_network(self, hidden_dim):\n",
    "        return nn.Sequential(\n",
    "            nn.Linear(self.state_dim + self.action_dim, hidden_dim[0]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim[0], hidden_dim[1]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim[1], 1)\n",
    "        )\n",
    "\n",
    "    # Define or build a policy network architecture\n",
    "    def build_policy_network(self, hidden_dim):\n",
    "        return nn.Sequential(\n",
    "            nn.Linear(self.state_dim, hidden_dim[0]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim[0], hidden_dim[1]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim[1], self.action_dim * 2) # mean and log_std\n",
    "        )\n",
    "\n",
    "    # Soft update function for target networks\n",
    "    def update_targets(self, target, source, tau):\n",
    "        for target_param, source_param in zip(target.parameters(), source.parameters()):\n",
    "            target_param.data.copy_(tau * source_param.data + (1.0 - tau) * target_param.data)\n",
    "\n",
    "    # Get action using the policy network\n",
    "    def get_action(self, state):\n",
    "        if len(state.shape) == 1:  # Handle single state (no batch dimension)\n",
    "            state = state.unsqueeze(0)# adding batch dimension \n",
    "\n",
    "        mean_log_std = self.policy_network(state)\n",
    "        mean, log_std = mean_log_std.chunk(2, dim=-1)\n",
    "        std = log_std.exp() ##computes the standard deviation\n",
    "        normal = torch.distributions.Normal(mean, std)\n",
    "        action = normal.rsample()# random samopling\n",
    "\n",
    "        # Assuming continuous action space \n",
    "        action = action.tanh()\n",
    "\n",
    "        return action.squeeze(0) if state.shape[0] == 1 else action\n",
    "\n",
    "    # SAC algorithm update step, this is training steps for SAC\n",
    "    def update(self, replay_buffer, batch_size, discount, tau, alpha):\n",
    "        states, actions, rewards, next_states, dones = replay_buffer.sample(batch_size)\n",
    "\n",
    "        states = torch.FloatTensor(states)\n",
    "        actions = torch.FloatTensor(actions)\n",
    "        rewards = torch.FloatTensor(rewards).unsqueeze(1)\n",
    "        next_states = torch.FloatTensor(next_states)\n",
    "        dones = torch.FloatTensor(dones).unsqueeze(1)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            next_actions = self.get_action(next_states)\n",
    "\n",
    "            # Ensure actions is a 2D tensor\n",
    "            if len(next_actions.shape) == 1:\n",
    "                next_actions = next_actions.unsqueeze(1)\n",
    "\n",
    "            next_q1_values = self.target_q1_network(torch.cat([next_states, next_actions], 1))\n",
    "            next_q2_values = self.target_q2_network(torch.cat([next_states, next_actions], 1))\n",
    "            next_q_values = torch.min(next_q1_values, next_q2_values)\n",
    "\n",
    "            # Compute the target Q-values\n",
    "            target_values = rewards + discount * (1 - dones) * next_q_values\n",
    "\n",
    "        # Ensure actions is a 2D tensor for concatenation\n",
    "        if len(actions.shape) == 1:\n",
    "            actions = actions.unsqueeze(1)\n",
    "\n",
    "        # Concatenate states and actions for Q value calculation\n",
    "        q1_values = self.q1_network(torch.cat([states, actions], 1))\n",
    "        q2_values = self.q2_network(torch.cat([states, actions], 1))\n",
    "\n",
    "        # Calculate the Q-networks loss\n",
    "        q1_loss = F.mse_loss(q1_values, target_values)\n",
    "        q2_loss = F.mse_loss(q2_values, target_values)\n",
    "\n",
    "        # Q-networks updates\n",
    "        q1_values = self.q1_network(torch.cat([states, actions], 1))\n",
    "        q2_values = self.q2_network(torch.cat([states, actions], 1))\n",
    "        q1_loss = F.mse_loss(q1_values, target_values)\n",
    "        q2_loss = F.mse_loss(q2_values, target_values)\n",
    "\n",
    "        self.q1_optimizer.zero_grad()\n",
    "        q1_loss.backward()\n",
    "        self.q1_optimizer.step()\n",
    "\n",
    "        self.q2_optimizer.zero_grad()\n",
    "        q2_loss.backward()\n",
    "        self.q2_optimizer.step()\n",
    "\n",
    "        # Policy network update\n",
    "        policy_actions = self.get_action(states)\n",
    "        q1_policy = self.q1_network(torch.cat([states, policy_actions], 1))\n",
    "        q2_policy = self.q2_network(torch.cat([states, policy_actions], 1))\n",
    "        policy_loss = -torch.min(q1_policy, q2_policy).mean()\n",
    "\n",
    "        self.policy_optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        self.policy_optimizer.step()\n",
    "\n",
    "        # Soft update of target networks\n",
    "        self.update_targets(self.target_q1_network, self.q1_network, tau)\n",
    "        self.update_targets(self.target_q2_network, self.q2_network, tau)\n",
    "        # Return average of Q1 and Q2 losses\n",
    "        return (q1_loss.item() + q2_loss.item()) / 2\n",
    "    \n",
    "#Below class ReplayBuffer to store experiences and sample them for training\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.buffer = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append(None)\n",
    "        self.buffer[self.position] = (state, action, reward, next_state, done)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        state, action, reward, next_state, done = map(np.stack, zip(*batch))\n",
    "        return state, action, reward, next_state, done\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "    \n",
    "\n",
    "# Parameters\n",
    "state_dim = ...  # Depends on the environment\n",
    "action_dim = ...  # Depends on the environment\n",
    "hidden_dim = (256, 256)\n",
    "alpha = 0.2\n",
    "lr = 3e-4\n",
    "buffer_capacity = 1000000\n",
    "batch_size = 256\n",
    "num_episodes = 50\n",
    "discount = 0.99\n",
    "tau = 0.005\n",
    "\n",
    "# Environment setup\n",
    "env = gym.make('MountainCarContinuous-v0')\n",
    "#state_dim = env.observation_space.shape[0]\n",
    "#action_dim = env.action_space.shape[0]\n",
    "state_dim = env.observation_space.shape[0]\n",
    "\n",
    "# For discrete action spaces, use the 'n' attribute instead of 'shape'\n",
    "if isinstance(env.action_space, gym.spaces.Discrete):\n",
    "    action_dim = env.action_space.n\n",
    "else:\n",
    "    action_dim = env.action_space.shape[0]\n",
    "\n",
    "# SAC agent\n",
    "#agent = SACAgent(state_dim, action_dim, hidden_dim, alpha, lr)\n",
    "agent = SACAgent(state_dim, action_dim, hidden_dim, alpha, lr)\n",
    "# Replay buffer\n",
    "replay_buffer = ReplayBuffer(buffer_capacity)\n",
    "\n",
    "# Training loop\n",
    "update_frequency = 10\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    episode_reward = 0\n",
    "    episode_losses = [] \n",
    "    \n",
    "    for t in range(1, 101):\n",
    "        # Select action\n",
    "        \n",
    "        env.render()\n",
    "       \n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "        action = agent.get_action(state_tensor)\n",
    "\n",
    "        # Convert action to numpy array and detach from the computation graph\n",
    "        action_np = action.detach().cpu().numpy()\n",
    "\n",
    "        # Check the type of action space and format the action accordingly\n",
    "        if isinstance(env.action_space, gym.spaces.Discrete):\n",
    "            # Use argmax for discrete action spaces\n",
    "            action_to_env = np.argmax(action_np)\n",
    "        else:\n",
    "            # Use the action as is for continuous action spaces\n",
    "            action_to_env = action_np\n",
    "\n",
    "        # Step in the environment with the correctly formatted action\n",
    "        next_state, reward, done, _ = env.step(action_to_env)\n",
    "       \n",
    "\n",
    "        # Ensure next_state is a 1D array\n",
    "        next_state = np.array(next_state, dtype=np.float32).flatten()\n",
    "\n",
    "        # Store in replay buffer\n",
    "        replay_buffer.push(state, action_np[0], reward, next_state, done)\n",
    "\n",
    "        # Update state\n",
    "        state = next_state\n",
    "        episode_reward += reward\n",
    "\n",
    "        if len(replay_buffer) > batch_size:\n",
    "            loss = agent.update(replay_buffer, batch_size, discount, tau, alpha)\n",
    "            episode_losses.append(loss)  # Add the loss to the list\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "        else:\n",
    "            # Update the state for the next step\n",
    "            state = next_state\n",
    "\n",
    "    # Calculate average loss for the episode\n",
    "    avg_loss = sum(episode_losses) / len(episode_losses) if episode_losses else 0\n",
    "\n",
    "    # Print episode information\n",
    "    #print(f\"Episode: {episode}, Reward: {episode_reward}\")\n",
    "    print(f\"Episode: {episode}, Reward: {episode_reward}, Average Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "# Print Q-network values (added code)\n",
    "    if episode % update_frequency == 0:\n",
    "        print(\"\\nQ-Values:\")\n",
    "        for s in range(10):  # Print Q-values for the first 10 states\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "            action = agent.get_action(state_tensor)\n",
    "\n",
    "            # Convert action to a numpy array\n",
    "            action_np = action.detach().cpu().numpy()\n",
    "\n",
    "            # Check the type of action space and format the action accordingly\n",
    "            if isinstance(env.action_space, gym.spaces.Discrete):\n",
    "                action_to_env = np.argmax(action_np)\n",
    "            else:\n",
    "                action_to_env = action_np\n",
    "\n",
    "            # Concatenate the state and action tensors\n",
    "            state_action_concat = torch.cat([state_tensor, torch.FloatTensor([action_to_env])], dim=1)\n",
    "\n",
    "            q1_value = agent.q1_network(state_action_concat).item()\n",
    "            q2_value = agent.q2_network(state_action_concat).item()\n",
    "            print(f\"State {s + 1}: Q1 = {q1_value:.4f}, Q2 = {q2_value:.4f}\")\n",
    "\n",
    "# Close the environment\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50d7362",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
